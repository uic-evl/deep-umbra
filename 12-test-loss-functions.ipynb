{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "from deep_shadow import *\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['nyc']\n",
    "dates = ['summer']\n",
    "\n",
    "zoom = 16\n",
    "shadow_path = 'data/shadows/'\n",
    "height_path = 'data/heights_new/'\n",
    "\n",
    "TILES_PER_CITY = 200\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep these images off train or test dataset\n",
    "path_list = ['16/19305/24641.png', \n",
    "             '16/19317/24654.png', \n",
    "             '16/19348/24629.png', \n",
    "             '16/19301/24621.png', \n",
    "             '16/19331/24653.png', \n",
    "             '16/19311/24658.png', \n",
    "             '16/19306/24625.png',\n",
    "             '16/19309/24670.png',\n",
    "             '16/19294/24661.png',\n",
    "             '16/19329/24624.png',\n",
    "             '16/19333/24635.png',\n",
    "             '16/19320/24627.png',\n",
    "             '16/19302/24622.png',\n",
    "             '16/19342/24644.png',\n",
    "             '16/19302/24621.png',\n",
    "             '16/19302/24645.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_train_test(\n",
    "    height_path, shadow_path, cities, dates, zoom, TILES_PER_CITY, BATCH_SIZE, ignore_images = path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude and date inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, dat = False, False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the generator architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_arch():\n",
    "    down_stack = [\n",
    "            downsample(64, 4, apply_batchnorm=False),  # (batch_size, 256, 256, 64)\n",
    "            downsample(128, 4),  # (batch_size, 128, 128, 128)\n",
    "            downsample(256, 4),  # (batch_size, 64, 64, 256)\n",
    "            downsample(512, 4),  # (batch_size, 32, 32, 512)\n",
    "            downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "            downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "            downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "            downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "            downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    return down_stack, up_stack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Store Handpicked Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_images(checkpoint_name, deep_shadow, path_list, _lat=True, _dat=True):\n",
    "    city, date, zoom = 'nyc', 'summer', 16\n",
    "\n",
    "    if not os.path.exists('data/compare_models/%s/' % checkpoint_name):\n",
    "        os.makedirs('data/compare_models/%s/' % checkpoint_name)\n",
    "\n",
    "    for path in path_list:\n",
    "        i, j = int(path.split('/')[1]), int(path.split('/')[2].split('.')[0])\n",
    "        img_no = '%d_%d' % (i, j)\n",
    "        plot_path = 'data/compare_models/%s/%s' % (checkpoint_name, img_no)\n",
    "        test_on_image(deep_shadow.generator, height_path, shadow_path, city, date, zoom, i, j, path=plot_path, save=True, lat=_lat, dat=_dat)\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models with different loss functions and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_names = ['_l1', '_l2', '_l1_smooth_loss', '_ssim', '_sobel', '_ssim_with_sobel', '_ssim_l1_smooth_loss', '_ssim_l1_loss', '_ssim_sobel_plus_l1', '_berhu']\n",
    "loss_functions = [[l1_loss], [l2_loss], [l1_smooth_loss], [ssim_loss], [sobel_loss], [ssim_loss, sobel_loss], [ssim_loss, l1_smooth_loss], [ssim_loss, l1_loss], [ssim_loss, sobel_loss, l1_loss], [berhu_loss]]\n",
    "\n",
    "for i in range(len(checkpoint_names)):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    down_stack, up_stack = get_generator_arch()\n",
    "    \n",
    "    train_dataset, test_dataset = get_train_test(\n",
    "    height_path, shadow_path, cities, dates, zoom, TILES_PER_CITY, BATCH_SIZE, ignore_images = path_list)\n",
    "\n",
    "    checkpoint_name = 'test' + checkpoint_names[i]\n",
    "    checkpoint_path = 'training_checkpoints/%s' % (checkpoint_name)\n",
    "    deep_shadow = DeepShadow(512, 512, down_stack, up_stack, latitude=lat, date=dat, loss_funcs=loss_functions[i], type='unet', attention=False, model_name=checkpoint_name)\n",
    "    # deep_shadow.fit(checkpoint_path, train_dataset, test_dataset, 40000)\n",
    "\n",
    "    deep_shadow.restore(checkpoint_path)\n",
    "    store_images(checkpoint_name, deep_shadow, path_list, _lat = lat, _dat = dat)\n",
    "    df = pd.read_csv('data/compare_models/results.csv')\n",
    "    rmses, maes, mses, ssims, sobels = get_metrics(test_dataset, deep_shadow.generator, latitude=lat, date=dat)\n",
    "    df.loc[len(df.index)] = [checkpoint_name, np.average(maes), np.average(mses), np.average(rmses), np.average(ssims), np.average(sobels)] \n",
    "    df.to_csv('data/compare_models/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual artifacts!! 19302_24621, 19302_24622, etc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
